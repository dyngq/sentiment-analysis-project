{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['word2vec-nlp-tutorial', 'glove6b100dtxt', 'imdb-review-dataset']\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from nltk.corpus import stopwords                # stopwords are removed from text to keep just useful info\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "\n",
    "import keras\n",
    "from keras.preprocessing.text import one_hot, Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Embedding, LSTM, SpatialDropout1D, Input, Bidirectional,Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from keras.regularizers import l2\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "      <th>file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test</td>\n",
       "      <td>Once again Mr. Costner has dragged out a movie...</td>\n",
       "      <td>neg</td>\n",
       "      <td>0_2.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test</td>\n",
       "      <td>This is an example of why the majority of acti...</td>\n",
       "      <td>neg</td>\n",
       "      <td>10000_4.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test</td>\n",
       "      <td>First of all I hate those moronic rappers, who...</td>\n",
       "      <td>neg</td>\n",
       "      <td>10001_1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test</td>\n",
       "      <td>Not even the Beatles could write songs everyon...</td>\n",
       "      <td>neg</td>\n",
       "      <td>10002_3.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test</td>\n",
       "      <td>Brass pictures (movies is not a fitting word f...</td>\n",
       "      <td>neg</td>\n",
       "      <td>10003_3.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type     ...              file\n",
       "0  test     ...           0_2.txt\n",
       "1  test     ...       10000_4.txt\n",
       "2  test     ...       10001_1.txt\n",
       "3  test     ...       10002_3.txt\n",
       "4  test     ...       10003_3.txt\n",
       "\n",
       "[5 rows x 4 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_train_data_labeled = pd.read_csv(\"../input/word2vec-nlp-tutorial/labeledTrainData.tsv\", header=0, delimiter=\"\\t\", quoting=3)\n",
    "raw_train_data_unlabeled = pd.read_csv(\"../input/word2vec-nlp-tutorial/unlabeledTrainData.tsv\", header=0, delimiter=\"\\t\", quoting=3)\n",
    "raw_test_data = pd.read_csv(\"../input/word2vec-nlp-tutorial/testData.tsv\", header=0, delimiter=\"\\t\", quoting=3)\n",
    "\n",
    "imdb_data = pd.read_csv('../input/imdb-review-dataset/imdb_master.csv',encoding=\"latin-1\", index_col=0)\n",
    "print(len(imdb_data))\n",
    "imdb_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>Once again Mr. Costner has dragged out a movie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>This is an example of why the majority of acti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>First of all I hate those moronic rappers, who...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>Not even the Beatles could write songs everyon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>Brass pictures (movies is not a fitting word f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                             review\n",
       "0        0.0  Once again Mr. Costner has dragged out a movie...\n",
       "1        0.0  This is an example of why the majority of acti...\n",
       "2        0.0  First of all I hate those moronic rappers, who...\n",
       "3        0.0  Not even the Beatles could write songs everyon...\n",
       "4        0.0  Brass pictures (movies is not a fitting word f..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_data = imdb_data.drop([\"type\",\"file\"],axis=1)\n",
    "imdb_data['sentiment'] = imdb_data['label'].map({\"neg\":0, \"pos\":1})\n",
    "imdb_data = imdb_data.drop([\"label\"],axis=1)\n",
    "imdb_data = imdb_data.dropna() # 删除缺失数据\n",
    "imdb_data = imdb_data[['sentiment','review']]\n",
    "\n",
    "imdb_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = raw_train_data_labeled['review']\n",
    "y = raw_train_data_labeled['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_data = X.append(raw_test_data['review'])\n",
    "review_data = review_data.append(imdb_data['review'])\n",
    "review_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntrain = X.shape[0]\n",
    "df = raw_train_data_labeled.append(raw_test_data, sort=False)\n",
    "df = df.drop(['sentiment'], axis=1)\n",
    "df_review = df['review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Here we will do preprocessing\n",
    "1. Removing punctuations\n",
    "2. Lowering all words\n",
    "3. removing non-alphabet things\n",
    "4. removing stop words\n",
    "5. Tokenizing the sentence\n",
    "'''\n",
    "import string\n",
    "\n",
    "review_lines = list()\n",
    "lines = review_data.values.tolist()\n",
    "\n",
    "for line in lines:\n",
    "    \n",
    "    '''\n",
    "    breaks line into it's sub parts like each word and comma etc,\n",
    "    https://pythonspot.com/tokenizing-words-and-sentences-with-nltk/\n",
    "    '''\n",
    "    tokens = word_tokenize(line)   \n",
    "    \n",
    "     #convert to lower case\n",
    "    tokens = [w.lower() for w in tokens]\n",
    "    \n",
    "    #remove punctuation from each word\n",
    "    # brief detail: https://pythonadventures.wordpress.com/2017/02/05/remove-punctuations-from-a-text/\n",
    "    table = str.maketrans('','', string.punctuation)\n",
    "    stripped = [w.translate(table) for w in tokens]\n",
    "     \n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    words = [w for w in stripped if w.isalpha()]\n",
    "    \n",
    "    # filter out stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [w for w in words if w not in stop_words]\n",
    "    \n",
    "    review_lines.append(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/smart_open/ssh.py:34: UserWarning: paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n",
      "  warnings.warn('paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 135639\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "gensim is python library for training word embeddings in given data\n",
    "for more information visit: \n",
    "1. https://machinelearningmastery.com/develop-word-embeddings-python-gensim/\n",
    "2. http://kavita-ganesan.com/gensim-word2vec-tutorial-starter-code/#.XEoWKVwzbIV\n",
    "'''\n",
    "import gensim\n",
    "\n",
    "embedding_vector_size = 150\n",
    "# now training embeddings for each word \n",
    "model_1 = gensim.models.Word2Vec(sentences = review_lines, size=embedding_vector_size, min_count=1, window=5, workers=4 )\n",
    "\n",
    "# to get total number of unique words\n",
    "words = list(model_1.wv.vocab)\n",
    "\n",
    "print(\"vocab size:\", len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "review_lines_1 = list()\n",
    "lines = X.values.tolist()\n",
    "\n",
    "for line in lines:\n",
    "    \n",
    "    '''\n",
    "    breaks line into it's sub parts like each word and comma etc,\n",
    "    https://pythonspot.com/tokenizing-words-and-sentences-with-nltk/\n",
    "    '''\n",
    "    tokens = word_tokenize(line)   \n",
    "    \n",
    "     #convert to lower case\n",
    "    tokens = [w.lower() for w in tokens]\n",
    "    \n",
    "    #remove punctuation from each word\n",
    "    # brief detail: https://pythonadventures.wordpress.com/2017/02/05/remove-punctuations-from-a-text/\n",
    "    table = str.maketrans('','', string.punctuation)\n",
    "    stripped = [w.translate(table) for w in tokens]\n",
    "     \n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    words = [w for w in stripped if w.isalpha()]\n",
    "    \n",
    "    # filter out stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [w for w in words if w not in stop_words]\n",
    "    \n",
    "    review_lines_1.append(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'frequency')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "leng=0\n",
    "length = [(leng + len(x)) for x in review_lines_1]\n",
    "plt.hist(length)\n",
    "plt.xlabel('length of words')\n",
    "plt.ylabel('frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123.5178\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "avg_length = sum(length)/len(review_lines_1)\n",
    "\n",
    "# if words are more than max_length then they are skipped, if less than padding with 0 is done\n",
    "print(avg_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#max_len = math.ceil(avg_length)             # this is used to decide how man words in seq to keep\n",
    "max_len = math.ceil(avg_length)\n",
    "'''\n",
    "Now we have trained the embeddings, we now have embedding vector for each word. We will\n",
    "convert our text training data to numeric using theseword embeddings.\n",
    "First, we need to make length of each input same, therefore we'll do padding. But padding happends \n",
    "on numeric data, therefore we'll convert texts to sequences using tokenize() function. Then add padding\n",
    "Then we'll replace each non-zero numeric resulted from texts to sequences to its corresponding word\n",
    "embedding.\n",
    "'''\n",
    "max_features = 6000\n",
    "tokenizer = Tokenizer(num_words=max_features)       #keeps 6000 most common words\n",
    "train_test_data = review_lines                   # contains word tokens extracted from lines\n",
    "tokenizer.fit_on_texts(train_test_data)\n",
    "sequence = tokenizer.texts_to_sequences(train_test_data)\n",
    "train_test_data = pad_sequences(sequence, maxlen = max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing embedding matrix\n",
    "vocab_size = len(tokenizer.word_index)+1\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_vector_size))\n",
    "# +1 is done because i starts from 1 instead of 0, and goes till len(vocab)\n",
    "for  word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = model_1.wv[word]\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../embedding_matrix_1.npy',embedding_matrix)\n",
    "np.save('embedding_matrix_1.npy',embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 6000\n",
    "tokenizer = Tokenizer(num_words=max_features)       #keeps 6000 most common words\n",
    "train_test_data = review_lines_1                   # contains word tokens extracted from lines\n",
    "tokenizer.fit_on_texts(train_test_data)\n",
    "sequence = tokenizer.texts_to_sequences(train_test_data)\n",
    "train_test_data = pad_sequences(sequence, maxlen = max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 124)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = train_test_data\n",
    "# X = X.reshape(-1,124)\n",
    "y1 = y\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000,)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ReduceLROnPlateau\n",
    "keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, verbose=0, mode='auto', epsilon=0.0001, cooldown=0, min_lr=0)\n",
    "当评价指标不在提升时，减少学习率\n",
    "\n",
    "当学习停滞时，减少2倍或10倍的学习率常常能获得较好的效果。该回调函数检测指标的情况，如果在patience个epoch中看不到模型性能提升，则减少学习率\n",
    "\n",
    "keras 中文文档\n",
    "https://keras-cn.readthedocs.io/en/latest/other/callbacks/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X , y1, test_size=0.2, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Sequential()\n",
    "\n",
    "# model.add(Embedding(input_dim = vocab_size, output_dim = embedding_vector_size, \n",
    "#                     input_length = max_len, weights = [embedding_matrix]))\n",
    "# model.add(Bidirectional(LSTM(64, dropout=0.25, recurrent_dropout=0.1)))\n",
    "# model.add(Dense(10))\n",
    "# model.add(Dropout(0.3))\n",
    "# model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n",
    "#                                             patience=3, \n",
    "#                                             verbose=1, \n",
    "#                                             factor=0.5, \n",
    "#                                             min_lr=0.00001)\n",
    "\n",
    "# model.compile(optimizer='RMSprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "# print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = model.fit(X_train, y_train, epochs = 30, batch_size = 512, validation_data=(X_test, y_test),callbacks = [learning_rate_reduction])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
