{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/opt/conda/lib/python3.6/site-packages/smart_open/ssh.py:34: UserWarning: paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n",
      "  warnings.warn('paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress')\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from nltk.corpus import stopwords                \n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "import re\n",
    "# from nltk.stem import WordNetLemmatizer # 同义词转述\n",
    "from nltk.stem import WordNetLemmatizer # 同义词转述\n",
    "\n",
    "import keras\n",
    "from keras import models\n",
    "from keras.preprocessing.text import one_hot, Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential,load_model\n",
    "from keras.layers import Dense, Flatten, Embedding, LSTM, SpatialDropout1D, Input, Bidirectional,Dropout\n",
    "from keras.layers import Dropout , Activation, GRU, TimeDistributed,Conv1D\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D,MaxPooling1D\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Convolution1D\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from keras.regularizers import l2\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from keras.engine.topology import Layer, InputSpec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imdb-review-dataset  kumarmanoj-bag-of-words-meets-bags-of-popcorn\r\n"
     ]
    }
   ],
   "source": [
    "!ls '../input/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n",
      "100000\n",
      "200000\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# print(os.listdir())\n",
    "\n",
    "raw_train_data_labeled = pd.read_csv(\"../input/kumarmanoj-bag-of-words-meets-bags-of-popcorn/labeledTrainData.tsv\", header=0, delimiter=\"\\t\", quoting=3)\n",
    "raw_train_data_unlabeled = pd.read_csv(\"../input/kumarmanoj-bag-of-words-meets-bags-of-popcorn/unlabeledTrainData.tsv\", header=0, delimiter=\"\\t\", quoting=3)\n",
    "raw_test_data = pd.read_csv(\"../input/kumarmanoj-bag-of-words-meets-bags-of-popcorn/testData.tsv\", header=0, delimiter=\"\\t\", quoting=3)\n",
    "\n",
    "imdb_data = pd.read_csv('../input/imdb-review-dataset/imdb_master.csv',encoding=\"latin-1\", index_col=0)\n",
    "print(len(imdb_data))\n",
    "\n",
    "imdb_data = imdb_data.drop([\"type\",\"file\"],axis=1)\n",
    "imdb_data['sentiment'] = imdb_data['label'].map({\"neg\":0, \"pos\":1})\n",
    "imdb_data = imdb_data.drop([\"label\",'sentiment'],axis=1)\n",
    "# imdb_data = imdb_data.dropna() # 删除缺失数据\n",
    "raw_train_data_labeled = raw_train_data_labeled.drop([\"id\",\"sentiment\"],axis=1)\n",
    "raw_train_data_unlabeled = raw_train_data_unlabeled.drop([\"id\"],axis=1)\n",
    "raw_test_data = raw_test_data.drop([\"id\"],axis=1)\n",
    "# print(raw_test_data.head())\n",
    "# = imdb_data['review']\n",
    "\n",
    "review_data = pd.concat([imdb_data,raw_train_data_unlabeled,raw_train_data_labeled,raw_test_data],ignore_index=True)\n",
    "print(len(imdb_data))\n",
    "print(len(review_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_data_2 = imdb_data['review'].append(raw_train_data_unlabeled['review'])\n",
    "review_data_2 = review_data_2.append(raw_train_data_labeled['review'])\n",
    "review_data_2 = review_data_2.append(raw_test_data['review'])\n",
    "\n",
    "type(review_data_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "review_lines = list()\n",
    "lines = review_data_2.values.tolist()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "# i = 1\n",
    "for line in lines:\n",
    "#     i = i + 1\n",
    "    '''\n",
    "    breaks line into it's sub parts like each word and comma etc,\n",
    "    https://pythonspot.com/tokenizing-words-and-sentences-with-nltk/\n",
    "    '''\n",
    "    tokens = word_tokenize(line)   \n",
    "#     print(tokens)\n",
    "     #convert to lower case\n",
    "    tokens = [w.lower() for w in tokens]\n",
    "    tokens = [lemmatizer.lemmatize(token, \"v\") for token in tokens] ## 同义词转述\n",
    "\n",
    "    #remove punctuation from each word\n",
    "    # brief detail: https://pythonadventures.wordpress.com/2017/02/05/remove-punctuations-from-a-text/\n",
    "    table = str.maketrans('','', string.punctuation)\n",
    "    stripped = [w.translate(table) for w in tokens]\n",
    "     \n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    words = [w for w in stripped if w.isalpha()]\n",
    "    \n",
    "    # filter out stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [w for w in words if ((w not in stop_words)and(w!='br')and(len(w)>1))]\n",
    "    \n",
    "    review_lines.append(words)\n",
    "    \n",
    "#     if (i>5):\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_2_vector is training ...\n",
      "word_2_vector finished training\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "\n",
    "embedding_vector_size = 100\n",
    "# now training embeddings for each word \n",
    "print('word_2_vector is training ...')\n",
    "model_word_2_vector = gensim.models.Word2Vec(sentences = review_lines, \n",
    "                                             size=embedding_vector_size, \n",
    "                                             min_count=1, \n",
    "                                             window=7, \n",
    "                                             workers=4, \n",
    "                                             sg = 1,\n",
    "                                            )\n",
    "print('word_2_vector finished training')\n",
    "# to get total number of unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_word_2_vector.save('../input/model_word_2_vector_02.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(review_lines)\n",
    "# review_lines[:2][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 193254\n"
     ]
    }
   ],
   "source": [
    "words = list(model_word_2_vector.wv.vocab)\n",
    "\n",
    "print(\"vocab size:\", len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n"
     ]
    }
   ],
   "source": [
    "review_lines = review_lines[150000:]\n",
    "print(len(review_lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117.8178\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#len(sequence)\n",
    "leng=0\n",
    "length = [(leng + len(x)) for x in review_lines]\n",
    "plt.hist(length)\n",
    "plt.xlabel('length of words')\n",
    "plt.ylabel('frequency')\n",
    "\n",
    "import math\n",
    "avg_length = sum(length)/len(review_lines)\n",
    "\n",
    "# if words are more than max_length then they are skipped, if less than padding with 0 is done\n",
    "print(avg_length)\n",
    "max_len = math.ceil(avg_length) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 6000\n",
    "tokenizer = Tokenizer(num_words=max_features)       #keeps 6000 most common words\n",
    "train_test_data = review_lines                       # contains word tokens extracted from lines\n",
    "tokenizer.fit_on_texts(train_test_data)\n",
    "sequence = tokenizer.texts_to_sequences(train_test_data)\n",
    "train_test_data = pad_sequences(sequence, maxlen = max_len)\n",
    "\n",
    "# Preparing embedding matrix\n",
    "vocab_size = len(tokenizer.word_index)+1\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_vector_size))\n",
    "# +1 is done because i starts from 1 instead of 0, and goes till len(vocab)\n",
    "for  word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = model_word_2_vector.wv[word]\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n",
      "25000\n"
     ]
    }
   ],
   "source": [
    "raw_train_data_labeled = pd.read_csv(\"../input/kumarmanoj-bag-of-words-meets-bags-of-popcorn/labeledTrainData.tsv\", header=0, delimiter=\"\\t\", quoting=3)\n",
    "raw_test_data = pd.read_csv(\"../input/kumarmanoj-bag-of-words-meets-bags-of-popcorn/testData.tsv\", header=0, delimiter=\"\\t\", quoting=3)\n",
    "\n",
    "\n",
    "print(len(raw_train_data_labeled))\n",
    "print(len(raw_test_data))\n",
    "\n",
    "X_train = train_test_data[5000:25000,:]\n",
    "X_test = train_test_data[:5000,:]\n",
    "# X = X.reshape(-1,123)\n",
    "y_train = raw_train_data_labeled['sentiment'][5000:25000]\n",
    "y_test = raw_train_data_labeled['sentiment'][:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttLayer, self).__init__(** kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape)==3\n",
    "        # W.shape = (time_steps, time_steps)\n",
    "        self.W = self.add_weight(name='att_weight', \n",
    "                                 shape=(input_shape[1], input_shape[1]),\n",
    "                                 initializer='uniform',\n",
    "                                 trainable=True)\n",
    "        self.b = self.add_weight(name='att_bias', \n",
    "                                 shape=(input_shape[1],),\n",
    "                                 initializer='uniform',\n",
    "                                 trainable=True)\n",
    "        super(AttLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # inputs.shape = (batch_size, time_steps, seq_len)\n",
    "        x = K.permute_dimensions(inputs, (0, 2, 1))\n",
    "        # x.shape = (batch_size, seq_len, time_steps)\n",
    "        a = K.softmax(K.tanh(K.dot(x, self.W) + self.b))\n",
    "        outputs = K.permute_dimensions(a * x, (0, 2, 1))\n",
    "        outputs = K.sum(outputs, axis=1)\n",
    "        return outputs\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], input_shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 118)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 118, 100)     12468000    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 118, 100)     30100       embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 118, 100)     30100       conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 118, 100)     30100       conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 118, 100)     0           conv1d_3[0][0]                   \n",
      "                                                                 embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 118, 200)     120600      add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "att_layer_1 (AttLayer)          (None, 200)          14042       bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            201         att_layer_1[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 12,693,143\n",
      "Trainable params: 12,693,143\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "sequence_input = Input(shape=(max_len,), dtype='int32')\n",
    "\n",
    "embedded_sequences = Embedding(vocab_size,\n",
    "                            embedding_vector_size,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=max_len,\n",
    "                            trainable= True )(sequence_input)\n",
    "\n",
    "conv1 = Conv1D(100,3,activation='relu',padding='same')(embedded_sequences)\n",
    "\n",
    "conv2 = Conv1D(100,3,activation='relu',padding='same')(conv1)\n",
    "\n",
    "conv3 = Conv1D(100,3,activation='relu',padding='same')(conv2)\n",
    "\n",
    "resnet = layers.add([conv3,embedded_sequences])\n",
    "\n",
    "gru = Bidirectional(GRU(100, dropout = 0.1, recurrent_dropout=0.1, return_sequences=True))(resnet)\n",
    "\n",
    "att = AttLayer()(gru)\n",
    "\n",
    "preds = Dense(1, activation='sigmoid')(att)\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n",
    "                                            patience=3, \n",
    "                                            verbose=1, \n",
    "                                            factor=0.5, \n",
    "                                            min_lr=0.00001)\n",
    "model.compile(optimizer='RMSprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "20000/20000 [==============================] - 56s 3ms/step - loss: 0.4384 - acc: 0.7966 - val_loss: 0.3109 - val_acc: 0.8738\n",
      "Epoch 2/10\n",
      "20000/20000 [==============================] - 53s 3ms/step - loss: 0.3066 - acc: 0.8733 - val_loss: 0.3128 - val_acc: 0.8682\n",
      "Epoch 3/10\n",
      "20000/20000 [==============================] - 51s 3ms/step - loss: 0.2630 - acc: 0.8931 - val_loss: 0.2868 - val_acc: 0.8826\n",
      "Epoch 4/10\n",
      "20000/20000 [==============================] - 51s 3ms/step - loss: 0.2304 - acc: 0.9098 - val_loss: 0.3465 - val_acc: 0.8586\n",
      "Epoch 5/10\n",
      "20000/20000 [==============================] - 52s 3ms/step - loss: 0.2064 - acc: 0.9203 - val_loss: 0.2677 - val_acc: 0.8922\n",
      "Epoch 6/10\n",
      "20000/20000 [==============================] - 51s 3ms/step - loss: 0.1828 - acc: 0.9295 - val_loss: 0.3237 - val_acc: 0.8742\n",
      "Epoch 7/10\n",
      "20000/20000 [==============================] - 52s 3ms/step - loss: 0.1589 - acc: 0.9412 - val_loss: 0.3948 - val_acc: 0.8512\n",
      "Epoch 8/10\n",
      "20000/20000 [==============================] - 53s 3ms/step - loss: 0.1381 - acc: 0.9506 - val_loss: 0.5279 - val_acc: 0.8234\n",
      "Epoch 9/10\n",
      "20000/20000 [==============================] - 50s 2ms/step - loss: 0.1142 - acc: 0.9616 - val_loss: 0.3584 - val_acc: 0.8818\n",
      "Epoch 10/10\n",
      "20000/20000 [==============================] - 51s 3ms/step - loss: 0.0891 - acc: 0.9697 - val_loss: 0.4883 - val_acc: 0.8580\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, \n",
    "                    y_train, epochs = 10, \n",
    "                    batch_size = 128, \n",
    "                    validation_data=(X_test, y_test),\n",
    "#                     callbacks = [learning_rate_reduction]\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model_1_w2v_simple.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sub = train_test_data[25000:,:]\n",
    "y_pred = model.predict(X_sub)\n",
    "y_pred = y_pred.reshape(-1,)\n",
    "# predictions = [1 if (x>0.5) else 0 for x in y_pred ]\n",
    "predictions = pd.Series(y_pred)\n",
    "ids = raw_test_data['id'].str.replace('\"', '')\n",
    "submission = pd.DataFrame({'id': ids, 'sentiment':predictions})\n",
    "submission.to_csv('submission.csv',index=False)\n",
    "y_preds = model.predict(X_test)\n",
    "y_preds = y_preds.reshape(-1,)\n",
    "x_stack = pd.Series(y_preds)\n",
    "\n",
    "y_stack = pd.Series(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_stack = pd.DataFrame({'x_stack': x_stack, 'y_stack':y_stack})\n",
    "sub_stack.to_csv('sub_stack.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200000"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
