{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/opt/conda/lib/python3.6/site-packages/smart_open/ssh.py:34: UserWarning: paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n",
      "  warnings.warn('paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress')\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import keras\n",
    "from nltk.corpus import stopwords                \n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "import re\n",
    "# from nltk.stem import WordNetLemmatizer # 同义词转述\n",
    "from nltk.stem import WordNetLemmatizer # 同义词转述\n",
    "\n",
    "import keras\n",
    "from keras import models\n",
    "from keras.preprocessing.text import one_hot, Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential,load_model\n",
    "from keras.layers import Dense, Flatten, Embedding, LSTM, SpatialDropout1D, Input, Bidirectional,Dropout\n",
    "from keras.layers import Dropout , Activation, GRU, TimeDistributed,Conv1D\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D,MaxPooling1D\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Convolution1D\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from keras.regularizers import l2\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glove6b100dtxt\tkumarmanoj-bag-of-words-meets-bags-of-popcorn\r\n"
     ]
    }
   ],
   "source": [
    "!ls '../input/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_x = pd.read_csv('../input/kumarmanoj-bag-of-words-meets-bags-of-popcorn/labeledTrainData.tsv', delimiter=\"\\t\")\n",
    "df_y = pd.read_csv('../input/kumarmanoj-bag-of-words-meets-bags-of-popcorn/testData.tsv', delimiter=\"\\t\")\n",
    "# df = df.drop(['id'], axis=1)\n",
    "# print(df_x.head())\n",
    "# print(df_y.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = df_x['sentiment']\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       id                                             review\n",
      "0  5814_8  With all this stuff going down at the moment w...\n",
      "1  2381_9  \\The Classic War of the Worlds\\\" by Timothy Hi...\n",
      "2  7759_3  The film starts with a manager (Nicholas Bell)...\n",
      "3  3630_4  It must be assumed that those who praised this...\n",
      "4  9495_8  Superbly trashy and wondrously unpretentious 8...\n",
      "         id                                             review\n",
      "0  12311_10  Naturally in a film who's main themes are of m...\n",
      "1    8348_2  This movie is a disaster within a disaster fil...\n",
      "2    5828_4  All in all, this is a movie for kids. We saw i...\n",
      "3    7186_2  Afraid of the Dark left me with the impression...\n",
      "4   12128_7  A very accurate depiction of small time mob li...\n"
     ]
    }
   ],
   "source": [
    "df_x = df_x.drop(['sentiment'],axis = 1)\n",
    "print(df_x.head())\n",
    "print(df_y.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat([df_x,df_y],ignore_index=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.stem import WordNetLemmatizer # 同义词转述\n",
    "from nltk.corpus import stopwords # 删除停用词\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\")) \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'[^\\w\\s]','',text, re.UNICODE)\n",
    "    text = re.sub(r\"\\\\\", \"\", text)    \n",
    "    text = re.sub(r\"\\'\", \"\", text)    \n",
    "    text = re.sub(r\"\\\"\", \"\", text)\n",
    "    text = re.sub(r\"<br />\", \"\", text) \n",
    "    text = text.lower()\n",
    "    text = [lemmatizer.lemmatize(token) for token in text.split(\" \")]\n",
    "    text = [lemmatizer.lemmatize(token, \"v\") for token in text]\n",
    "    text = [word for word in text if  (word not in stop_words)]\n",
    "#     and(word!='<br />')\n",
    "    text = \" \".join(text)\n",
    "    return text\n",
    "\n",
    "df['Processed_Reviews'] = df.review.apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['stuff go moment mj ive start listen music watch odd documentary watch wiz watch moonwalker maybe want get certain insight guy think wa really cool eighty maybe make mind whether guilty innocent moonwalker part biography part feature film remember go see cinema wa originally release ha subtle message mjs feel towards press also obvious message drug bad mkaybr br visually impressive course michael jackson unless remotely like mj anyway go hate find bore may call mj egotist consent make movie mj fan would say make fan true really nice himbr br actual feature film bite finally start 20 minute exclude smooth criminal sequence joe pesci convince psychopathic powerful drug lord want mj dead bad beyond mj overhear plan nah joe pescis character rant want people know supply drug etc dunno maybe hate mjs musiclots cool thing like mj turn car robot whole speed demon sequence. also, director must patience saint come film kiddy bad sequence usually director hate work one kid let alone whole bunch perform complex dance scene.bottom line, movie people like mj one level another (which think people). not, stay away. doe try give wholesome message ironically mjs bestest buddy movie girl! michael jackson truly one talented people ever grace planet guilty? well, attention ive give subject....hmmm well dont know people different behind close doors, know fact. either extremely nice stupid guy one sickest liars. hope latter.',\n",
       "       'classic war world timothy hines entertain film obviously go great effort length faithfully recreate h g well classic book mr hines succeed watch film appreciate fact wa standard predictable hollywood fare come every year eg spielberg version tom cruise slightest resemblance book obviously everyone look different thing movie envision amateur critic look criticize everything others rate movie important baseslike entertain people never agree critic enjoy effort mr hines put faithful hg. well classic novel, find entertaining. make easy overlook critic perceive shortcomings.'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.to_excel('df_test.xls')\n",
    "df['Processed_Reviews'].values[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 100)       26935\n",
      "[100, 200)     15237\n",
      "[200, 300)      4727\n",
      "[300, 400)      1795\n",
      "[400, 500)       857\n",
      "[500, 600)       410\n",
      "[600, 700)        22\n",
      "[700, 800)         5\n",
      "[900, 1000)        4\n",
      "[800, 900)         4\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fbd8a9e9828>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_len = df.Processed_Reviews.apply(lambda x: len(x.split(\" \")))\n",
    "\n",
    "data_len_list = data_len.values.tolist()\n",
    "# data_len_list\n",
    "import matplotlib as plt\n",
    "\n",
    "fenbu_list = [0,100,200,300,400,500,600,700,800,900,1000]\n",
    "fenbu_cut = pd.cut(data_len_list,fenbu_list,right=False)\n",
    "# print(fenbu_cut)\n",
    "fenbu = pd.value_counts(fenbu_cut)\n",
    "print(fenbu)\n",
    "\n",
    "fenbu.plot(kind='bar')\n",
    "# plt.xlabel('words_long_area')  \n",
    "# plt.ylabel('counts') \n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 300)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_features = 10000\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(df['Processed_Reviews'])\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(df['Processed_Reviews'])\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "maxlen = 300\n",
    "X_t = pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "X_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = X_t[:5000]\n",
    "y_val = y[:5000]\n",
    "X_test = X_t[25000:]\n",
    "X_t = X_t[5000:25000]\n",
    "y = y[5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 300)\n",
      "(5000,)\n",
      "(20000, 300)\n",
      "(20000,)\n",
      "(25000, 300)\n"
     ]
    }
   ],
   "source": [
    "print(x_val.shape)\n",
    "print(y_val.shape)\n",
    "print(X_t.shape)\n",
    "print(y.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "GLOVE_DIR = \"../input/glove6b100dtxt/\"\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Total %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 100\n",
    "maxlen = 300\n",
    "\n",
    "embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=maxlen,\n",
    "                            trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttLayer, self).__init__(** kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape)==3\n",
    "        # W.shape = (time_steps, time_steps)\n",
    "        self.W = self.add_weight(name='att_weight', \n",
    "                                 shape=(input_shape[1], input_shape[1]),\n",
    "                                 initializer='uniform',\n",
    "                                 trainable=True)\n",
    "        self.b = self.add_weight(name='att_bias', \n",
    "                                 shape=(input_shape[1],),\n",
    "                                 initializer='uniform',\n",
    "                                 trainable=True)\n",
    "        super(AttLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # inputs.shape = (batch_size, time_steps, seq_len)\n",
    "        x = K.permute_dimensions(inputs, (0, 2, 1))\n",
    "        # x.shape = (batch_size, seq_len, time_steps)\n",
    "        a = K.softmax(K.tanh(K.dot(x, self.W) + self.b))\n",
    "        outputs = K.permute_dimensions(a * x, (0, 2, 1))\n",
    "        outputs = K.sum(outputs, axis=1)\n",
    "        return outputs\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], input_shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KMaxPooling(Layer):\n",
    "    \"\"\"\n",
    "    k-max-pooling\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, k=1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.input_spec = InputSpec(ndim=3)\n",
    "        self.k = k\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], (input_shape[2] * self.k))\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # swap last two dimensions since top_k will be applied along the last dimension\n",
    "        shifted_input = tf.transpose(inputs, [0, 2, 1])\n",
    "\n",
    "        # extract top_k, returns two tensors [values, indices]\n",
    "        top_k = tf.nn.top_k(shifted_input, k=self.k, sorted=True, name=None)[0]\n",
    "\n",
    "        # return flattened output\n",
    "        return Flatten()(top_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 300, 100)          14582500  \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 298, 8)            2408      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 149, 8)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 147, 8)            200       \n",
      "_________________________________________________________________\n",
      "k_max_pooling_1 (KMaxPooling (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 8)                 32        \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 14,585,149\n",
      "Trainable params: 14,585,133\n",
      "Non-trainable params: 16\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "sequence_input = Input(shape=(maxlen,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "# l_gru = Bidirectional(GRU(100, return_sequences=True))(embedded_sequences)\n",
    "# l_att = AttLayer()(l_gru)\n",
    "# preds = Dense(1, activation='sigmoid')(l_att)\n",
    "# model = Model(sequence_input, preds)\n",
    "# model.compile(loss='binary_crossentropy',\n",
    "#               optimizer='rmsprop',\n",
    "#               metrics=['acc'])\n",
    "# model.summary()\n",
    "\n",
    "\n",
    "l_conv1 = Conv1D(8,3,activation='relu')(embedded_sequences)\n",
    "l_pool1 = MaxPooling1D(2)(l_conv1)\n",
    "# l_drop = Dropout(0.1)\n",
    "l_conv2 = Conv1D(8,3,activation='relu')(l_pool1)\n",
    "l_pool2 = KMaxPooling()(l_conv2)\n",
    "# l_conv2 = Conv1D(32,5,activation='relu')(l_pool2)\n",
    "# l_gru = Bidirectional(GRU(64, dropout = 0.1, recurrent_dropout=0.1, return_sequences=True))(l_conv2)\n",
    "# l_att = AttLayer()(l_gru)\n",
    "# l_drop = Dropout(0.1)\n",
    "# l_den = Dense(10, activation='relu')(l_pool2)\n",
    "l_bn = keras.layers.normalization.BatchNormalization()(l_pool2)\n",
    "preds = Dense(1, activation='sigmoid')(l_bn)\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n",
    "                                            patience=3, \n",
    "                                            verbose=1, \n",
    "                                            factor=0.5, \n",
    "                                            min_lr=0.00001)\n",
    "model.compile(optimizer='RMSprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model fitting - attention GRU network\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:2: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/3\n",
      "20000/20000 [==============================] - 9s 452us/step - loss: 0.5806 - acc: 0.6896 - val_loss: 0.5029 - val_acc: 0.7508\n",
      "Epoch 2/3\n",
      "20000/20000 [==============================] - 7s 346us/step - loss: 0.3901 - acc: 0.8297 - val_loss: 0.3957 - val_acc: 0.8252\n",
      "Epoch 3/3\n",
      "20000/20000 [==============================] - 7s 347us/step - loss: 0.3130 - acc: 0.8666 - val_loss: 0.3864 - val_acc: 0.8294\n"
     ]
    }
   ],
   "source": [
    "print(\"model fitting - attention GRU network\")\n",
    "history = model.fit(X_t, y, validation_data=(x_val, y_val),nb_epoch=3, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model_1_w2v_simple.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sub = X_test\n",
    "y_pred = model.predict(X_sub)\n",
    "# predictions = [1 if (x>0.5) else 0 for x in y_pred]\n",
    "y_pred = y_pred.reshape(-1,)\n",
    "predictions = pd.Series(y_pred)\n",
    "# X_sub = X_test\n",
    "# y_pred = model.predict(X_sub)\n",
    "# predictions = [1 if (x>0.5) else 0 for x in y_pred]\n",
    "# predictions = pd.Series(y_pred)\n",
    "ids = df_y['id'].str.replace('\"', '')\n",
    "submission = pd.DataFrame({'id': ids, 'sentiment':predictions})\n",
    "submission.to_csv('submission.csv',index=False)\n",
    "\n",
    "y_preds = model.predict(x_val)\n",
    "# x_stack = [1 if (x>0.5) else 0 for x in y_preds]\n",
    "y_preds = y_preds.reshape(-1,)\n",
    "x_stack = pd.Series(y_preds)\n",
    "\n",
    "y_stack = pd.Series(y_val)\n",
    "sub_stack = pd.DataFrame({'x_stack': x_stack, 'y_stack':y_stack})\n",
    "sub_stack.to_csv('sub_stack.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
